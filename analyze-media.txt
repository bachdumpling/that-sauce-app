import os
from typing import Dict, Optional, List
import google.generativeai as genai
import httpx
from motor.motor_asyncio import AsyncIOMotorClient
from datetime import datetime, UTC
import logging
import time
import asyncio
from concurrent.futures import ThreadPoolExecutor
import tempfile
from dotenv import load_dotenv
from rate_limiter import ContentType, GeminiHandler

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(f'analyzer_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler(),
    ],
)
logger = logging.getLogger(__name__)


class ImageAnalyzer:
    def __init__(self, get_api_key_func):
        load_dotenv()
        mongodb_uri = os.getenv("MONGODB_URI")

        if not mongodb_uri:
            raise ValueError("Please set MONGODB_URI in your .env file")

        # Initialize MongoDB with Motor
        self.client = AsyncIOMotorClient(mongodb_uri)
        self.db = self.client.muse_db

        # Initialize Gemini handler - this now handles rate limiting internally
        self.gemini = GeminiHandler(get_api_key_func)

        # Initialize thread pool
        self.executor = ThreadPoolExecutor(max_workers=10)

        # Analysis prompt
        self.prompt = """
        As a professional creative curator, provide a detailed analysis of this image focusing on:

        1. Technical Aspects:
           - Composition and framing
           - Lighting techniques used
           - Color palette and treatment
           - Technical quality and execution

        2. Creative Elements:
           - Main subject and focal points
           - Style and artistic approach
           - Mood and atmosphere
           - Visual storytelling elements

        3. Professional Context:
           - Apparent purpose or commercial application
           - Target audience or market segment
           - Production value indicators
           - Industry-specific elements

        Keep the description objective, detailed, and focused on visually present elements.
        Use professional terminology while remaining clear and precise.
        Avoid assumptions about intent and focus on observable qualities.
        Format as a continuous paragraph without headers or sections.
        """

    def get_highest_res_url(self, image: Dict) -> str:
        """Get the highest resolution URL from image resolutions"""
        if "resolutions" in image:
            res_dict = image["resolutions"]
            sizes = [int(key) for key in res_dict.keys() if key.isdigit()]
            if sizes:
                max_size = str(max(sizes))
                return res_dict[max_size]
        return image["url"]

    async def analyze_image(
        self, image_url: str, max_retries: int = 3
    ) -> Optional[str]:
        """Analyze image using Gemini Vision API"""
        retries = 0
        while retries < max_retries:
            try:
                async with httpx.AsyncClient(
                    follow_redirects=True, timeout=30.0
                ) as client:
                    response = await client.get(image_url)
                    if response.status_code != 200:
                        raise Exception(
                            f"Failed to download image: {response.status_code}"
                        )

                    with tempfile.NamedTemporaryFile(
                        suffix=".gif", delete=False
                    ) as temp_file:
                        temp_file.write(response.content)
                        temp_file_path = temp_file.name

                    try:
                        # Load image file
                        image_file = genai.upload_file(temp_file_path)

                        # Use gemini handler to generate content
                        response = await self.gemini.generate_content(
                            prompt=self.prompt,
                            context=image_file,  # Pass the image file directly
                            content_type=ContentType.IMAGE,
                        )

                        # If we get a response, return it
                        if response:
                            return response.strip()

                    finally:
                        os.unlink(temp_file_path)

            except Exception as e:
                retries += 1
                if retries == max_retries:
                    logger.error(f"Max retries reached for image analysis: {str(e)}")
                    return None
                await asyncio.sleep(1)

        return None

    async def generate_embedding(self, text: str) -> Optional[List[float]]:
        """Generate embedding using Gemini handler"""
        try:
            input_text = text["description"] if isinstance(text, dict) else text

            # Use gemini handler for embeddings - it handles rate limiting internally
            embedding = await self.gemini.generate_embedding(
                content=input_text, content_type=ContentType.IMAGE
            )

            return embedding

        except Exception as e:
            logger.error(f"Error generating embedding: {str(e)}")
            return None

    async def process_image_batch(
        self, images: List[Dict], project_title: str
    ) -> List[bool]:
        """Process a batch of images concurrently"""
        results = []
        for image in images:
            success = False
            try:
                if not image.get("ai_analysis") and not image.get("analysis_error"):
                    image_url = self.get_highest_res_url(image)
                    description = await self.analyze_image(image_url)

                    if description:
                        await self.db.images.update_one(
                            {"image_id": image["image_id"]},
                            {"$set": {"ai_analysis": {"description": description}}},
                        )
                        image["ai_analysis"] = {"description": description}
                    else:
                        await self.db.images.update_one(
                            {"image_id": image["image_id"]},
                            {"$set": {"analysis_error": True}},
                        )
                        continue

                if image.get("ai_analysis") and not image.get("embedding"):
                    embedding = await self.generate_embedding(image["ai_analysis"])
                    if embedding:
                        await self.db.images.update_one(
                            {"image_id": image["image_id"]},
                            {
                                "$set": {
                                    "embedding": embedding,
                                    "analyzed_at": datetime.now(UTC),
                                }
                            },
                        )
                        success = True

            except Exception as e:
                logger.error(f"Error processing image: {str(e)}")
                success = False

            results.append(success)

        return results

    async def analyze_portfolio(self, username: str) -> Dict[str, int]:
        """Analyze all images in a creator's portfolio"""
        try:
            # Get creator
            creator = await self.db.creators.find_one({"username": username})
            if not creator:
                return {"error": "Creator not found"}

            # Get portfolio
            portfolio = await self.db.portfolios.find_one(
                {"creator_id": creator["creator_id"]}
            )
            if not portfolio:
                return {"error": "Portfolio not found"}

            stats = {"success": 0, "failed": 0, "skipped": 0}

            # Process each project
            for project_id in portfolio["project_ids"]:
                project = await self.db.projects.find_one({"project_id": project_id})
                if not project:
                    stats["failed"] += 1
                    continue

                # Modified query to correctly find unprocessed images
                cursor = self.db.images.find(
                    {
                        "project_id": project_id,
                        "$or": [
                            {"ai_analysis": {"$exists": False}},  # No ai_analysis field
                            {"ai_analysis": None},  # ai_analysis is null
                            {"embedding": {"$exists": False}},  # No embedding field
                            {"embedding": None},  # embedding is null
                        ],
                    }
                )

                images = await cursor.to_list(length=None)

                if not images:
                    logger.info(
                        f"No unprocessed images found for project: {project['project_title']}"
                    )
                    stats["skipped"] += 1
                    continue

                # Process images in batches
                batch_size = 50
                for i in range(0, len(images), batch_size):
                    batch = images[i : i + batch_size]
                    results = await self.process_image_batch(
                        batch, project["project_title"]
                    )
                    stats["success"] += sum(1 for r in results if r)
                    stats["failed"] += sum(1 for r in results if not r)

            return stats

        except Exception as e:
            logger.error(f"Error analyzing portfolio: {str(e)}")
            return {"error": str(e)}


async def main(single_username: str = None):
    try:
        analyzer = ImageAnalyzer()

        if single_username:
            # Process single creator
            logger.info(f"\nProcessing single creator: {single_username}")
            stats = await analyzer.analyze_portfolio(single_username)

            if "error" in stats:
                logger.error(f"Analysis failed for {single_username}: {stats['error']}")
            else:
                logger.info(f"Results for {single_username}:")
                logger.info(f"Successfully analyzed images: {stats['success']}")
                logger.info(f"Failed images: {stats['failed']}")
                logger.info(f"Skipped (already analyzed): {stats['skipped']}")
        else:
            # Process all creators
            cursor = analyzer.db.creators.find({})
            creators = await cursor.to_list(length=None)

            total_creators = len(creators)
            logger.info(f"Found {total_creators} creators to process")

            for idx, creator in enumerate(creators, 1):
                username = creator["username"]
                logger.info(f"\nProcessing creator {idx}/{total_creators}: {username}")

                stats = await analyzer.analyze_portfolio(username)

                if "error" in stats:
                    logger.error(f"Analysis failed for {username}: {stats['error']}")
                else:
                    logger.info(f"Results for {username}:")
                    logger.info(f"Successfully analyzed images: {stats['success']}")
                    logger.info(f"Failed images: {stats['failed']}")
                    logger.info(f"Skipped (already analyzed): {stats['skipped']}")

                await asyncio.sleep(1)

    except Exception as e:
        logger.error(f"Main program error: {str(e)}")


if __name__ == "__main__":
    import sys

    username = sys.argv[1] if len(sys.argv) > 1 else None
    asyncio.run(main(username))

import { GoogleGenerativeAI } from "@google/generative-ai";
import { GoogleAIFileManager, FileState } from "@google/generative-ai/server";
import axios from "axios";
import fs from "fs";
import path from "path";
import cliProgress from "cli-progress";
import dotenv from "dotenv";
import { mkdir } from "fs/promises";
import { exec } from "child_process";
import { promisify } from "util";
import { SchemaType } from "@google/generative-ai/server";

dotenv.config();

const execAsync = promisify(exec);

// Configure Gemini API
const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);

/**
 * Downloads a video from a URL using yt-dlp
 * @param {string} url - The URL of the video to download
 * @param {string} outputPath - The path where the video will be saved
 * @returns {Promise<string|null>} - The path to the downloaded video or null if failed
 */
async function downloadVideo(url, outputPath = "temp_video.mp4") {
  try {
    console.log("Starting download from URL:", url);

    // Configure yt-dlp options similar to Python version
    const options = [
      "--format",
      "bestvideo[height=360]+bestaudio/best",
      "--merge-output-format",
      "mp4",
      "--quiet",
      "--no-playlist",
      "--ignore-errors",
      "-o",
      outputPath,
    ];

    // Execute yt-dlp command
    console.log("Executing yt-dlp command...");
    const { stdout, stderr } = await execAsync(
      `yt-dlp ${options.join(" ")} "${url}"`
    );

    if (stderr) {
      console.log("yt-dlp stderr:", stderr);
    }
    if (stdout) {
      console.log("yt-dlp stdout:", stdout);
    }

    // Verify the downloaded file
    const stats = await fs.promises.stat(outputPath);
    console.log(
      "Download complete. File size:",
      (stats.size / 1024 / 1024).toFixed(2),
      "MB"
    );

    if (stats.size === 0) {
      throw new Error("Downloaded file is empty");
    }

    console.log("Video downloaded successfully");
    return outputPath;
  } catch (error) {
    console.error("Error downloading video:", error.message);
    return null;
  }
}

/**
 * Generates an embedding vector using Gemini API
 * @param {string|Object} text - The text or object containing description to generate embedding for
 * @returns {Promise<number[]|null>} - The embedding vector
 */
async function generateEmbedding(text) {
  try {
    console.log("\n=== Starting Embedding Generation ===");

    // Handle input text format
    const inputText = typeof text === "object" ? text.description : text;

    // Initialize Gemini model for embeddings
    const model = genAI.getGenerativeModel({
      model: "models/text-embedding-004",
    });

    // Generate embedding
    const result = await model.embedContent(inputText);

    // Check if embedding exists in response
    if (!result?.embedding?.values) {
      throw new Error("No embedding values in response");
    }

    const embedding = Array.from(result.embedding.values);
    console.log("First 5 values:", embedding.slice(0, 5));

    return embedding;
  } catch (error) {
    console.error("\n=== Embedding Generation Error ===");
    console.error("Error type:", error.constructor.name);
    console.error("Error message:", error.message);
    console.error("Error stack:", error.stack);
    return null;
  }
}

/**
 * Analyzes a video using Gemini API
 * @param {string} videoPath - Path to the video file
 * @returns {Promise<Object|null>} - Analysis results and embedding
 */
async function analyzeVideo(videoPath) {
  try {
    console.log("\nReading video file...");
    console.log("Video path:", videoPath);

    // Verify file exists and check size
    const stats = await fs.promises.stat(videoPath);
    console.log("File size:", (stats.size / 1024 / 1024).toFixed(2), "MB");

    if (stats.size === 0) {
      throw new Error("Video file is empty");
    }

    // Read first few bytes to verify it's a valid MP4
    const buffer = Buffer.alloc(8);
    const fd = await fs.promises.open(videoPath, "r");
    await fd.read(buffer, 0, 8, 0);
    await fd.close();

    // Check MP4 signature (ftyp)
    if (buffer.toString("hex", 4, 8) !== "66747970") {
      console.error("File signature:", buffer.toString("hex"));
      throw new Error("Invalid MP4 file format");
    }

    console.log(
      "Initializing file manager with API key length:",
      process.env.GEMINI_API_KEY?.length
    );
    const fileManager = new GoogleAIFileManager(process.env.GEMINI_API_KEY);

    // Upload with retry logic
    let uploadResponse;
    const maxRetries = 3;
    for (let i = 0; i < maxRetries; i++) {
      try {
        console.log(`Upload attempt ${i + 1}/${maxRetries}...`);
        uploadResponse = await fileManager.uploadFile(videoPath, {
          mimeType: "video/mp4",
          displayName: "Video Analysis",
        });
        break;
      } catch (error) {
        console.error(`Upload attempt ${i + 1} failed:`, error);
        if (i === maxRetries - 1) throw error;
        await new Promise((resolve) => setTimeout(resolve, 5000)); // Wait 5s before retry
      }
    }

    console.log("Upload response:", JSON.stringify(uploadResponse, null, 2));

    // Wait for processing
    console.log("\nWaiting for video processing...");
    let file = await fileManager.getFile(uploadResponse.file.name);
    console.log("Initial file state:", file.state);

    let attempts = 0;
    const maxAttempts = 30; // 5 minutes maximum wait time

    while (file.state === FileState.PROCESSING && attempts < maxAttempts) {
      process.stdout.write(".");
      attempts++;
      await new Promise((resolve) => setTimeout(resolve, 10000)); // Wait 10 seconds
      file = await fileManager.getFile(uploadResponse.file.name);
      console.log(`\nAttempt ${attempts}: File state:`, file.state);
    }

    if (file.state === FileState.FAILED) {
      throw new Error(`Video processing failed. Final state: ${file.state}`);
    }

    if (attempts >= maxAttempts) {
      throw new Error("Video processing timed out after 5 minutes");
    }

    console.log("\nFile processing complete!");
    console.log("Final file data:", JSON.stringify(file, null, 2));

    console.log("\nInitializing Gemini model...");
    const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });

    const prompt = `Analyze this video using the following JSON schema:

    VideoAnalysis = {
      'description': string  // Detailed analysis of the video including visual style, technical quality, storytelling, production value, and creative techniques
    }

    Return: VideoAnalysis`;

    console.log("\nGenerating analysis...");
    const result = await model.generateContent([
      {
        fileData: {
          mimeType: file.mimeType,
          fileUri: file.uri,
        },
      },
      { text: prompt },
    ]);

    console.log("Raw response:", result);
    const response = await result.response;
    const responseText = response.text();
    console.log("Response text:", responseText);

    // Clean up the response text by removing markdown code blocks
    const cleanedText = responseText
      .replace(/```json\n/g, "") // Remove opening ```json
      .replace(/```\n/g, "") // Remove opening ```
      .replace(/\n```/g, "") // Remove closing ```
      .trim(); // Remove any extra whitespace

    // Parse the JSON response
    const analysis = JSON.parse(cleanedText);
    console.log("\nAnalysis generated successfully:");
    console.log(JSON.stringify(analysis, null, 2));

    // Generate embedding from the description string
    const embedding = await generateEmbedding(analysis.description);

    return {
      analysis,
      embedding,
      metadata: {
        timestamp: new Date().toISOString(),
        videoPath: videoPath,
        embeddingDimensions: embedding ? embedding.length : null,
      },
    };
  } catch (error) {
    console.error("Error analyzing video. Full error:", error);
    console.error("Error stack:", error.stack);
    return null;
  } finally {
    // Cleanup
    if (videoPath) {
      try {
        await fs.promises.unlink(videoPath);
        console.log("\nTemporary video file cleaned up");
      } catch (error) {
        console.error("Error cleaning up temporary file:", error.message);
      }
    }
  }
}

/**
 * Main function to orchestrate the video analysis process
 */
async function main() {
  const videoUrl = process.env.VIDEO_URL || "https://example.com/video.mp4";

  console.log("Starting video analysis process...");
  console.log("Video URL:", videoUrl);

  // Download video
  const videoPath = await downloadVideo(videoUrl);
  if (!videoPath) {
    console.error("Failed to download video");
    return;
  }

  // Analyze video and generate embedding
  const results = await analyzeVideo(videoPath);
  if (results) {
    console.log("\nFinal Results:");
    console.log("=".repeat(50));
    console.log("Analysis:", JSON.stringify(results.analysis, null, 2));

    // Check if embedding exists before trying to slice it
    if (results.embedding) {
      console.log(
        "\nEmbedding (first 5 dimensions):",
        results.embedding.slice(0, 5)
      );
    } else {
      console.log("\nWarning: No embedding generated");
    }

    console.log("\nMetadata:", JSON.stringify(results.metadata, null, 2));

    // Save results to file
    const outputPath = "analysis_results.json";
    await fs.promises.writeFile(outputPath, JSON.stringify(results, null, 2));
    console.log(`\nResults saved to ${outputPath}`);
  }
}

async function cleanupAndExit(videoPath) {
  if (videoPath) {
    try {
      await fs.promises.unlink(videoPath);
      console.log("Cleaned up video file during exit");
    } catch (error) {
      console.error("Error cleaning up video file during exit:", error.message);
    }
  }
  process.exit(1);
}

// Handle process termination
process.on("SIGINT", () => cleanupAndExit());
process.on("SIGTERM", () => cleanupAndExit());
process.on("uncaughtException", (error) => {
  console.error("Uncaught Exception:", error);
  cleanupAndExit();
});

// Run the program
main().catch(async (error) => {
  console.error("Error in main program:", error);
  await cleanupAndExit();
});

import time
import logging
from typing import Optional, Dict, List, Union
import google.generativeai as genai
from enum import Enum
import asyncio

logger = logging.getLogger(__name__)


class ContentType(Enum):
    IMAGE = "image"
    PROJECT = "project"
    PORTFOLIO = "portfolio"
    CREATOR = "creator"  # Added creator type


class RateLimiter:
    def __init__(self):
        self.request_times = {}
        self.daily_request_count = 0
        self.last_reset_time = time.time()

        # Rate limiting constants
        self.VISION_CALLS_PER_MINUTE = 15
        self.EMBEDDING_CALLS_PER_MINUTE = 1500
        self.CALLS_PER_DAY = 1500
        self.SECONDS_PER_MINUTE = 60
        self.SECONDS_PER_DAY = 86400

    def _initialize_content_type(self, content_type: ContentType):
        if content_type not in self.request_times:
            self.request_times[content_type] = []

    def can_make_request(self, content_type: ContentType) -> bool:
        self._initialize_content_type(content_type)
        current_time = time.time()

        # Daily reset
        if current_time - self.last_reset_time >= self.SECONDS_PER_DAY:
            self.daily_request_count = 0
            self.last_reset_time = current_time

        # Vision API daily limit check
        if (
            content_type == ContentType.IMAGE
            and self.daily_request_count >= self.CALLS_PER_DAY
        ):
            return False

        # Clean up old request times
        minute_ago = current_time - self.SECONDS_PER_MINUTE
        self.request_times[content_type] = [
            t for t in self.request_times[content_type] if t > minute_ago
        ]

        limit = (
            self.VISION_CALLS_PER_MINUTE
            if content_type == ContentType.IMAGE
            else self.EMBEDDING_CALLS_PER_MINUTE
        )
        return len(self.request_times[content_type]) < limit

    def record_request(self, content_type: ContentType):
        self._initialize_content_type(content_type)
        self.request_times[content_type].append(time.time())
        if content_type == ContentType.IMAGE:
            self.daily_request_count += 1

    def wait_if_needed(self, content_type: ContentType):
        while not self.can_make_request(content_type):
            wait_time = 1 if content_type != ContentType.IMAGE else 5
            logger.info(
                f"Rate limit reached for {content_type.value}, waiting {wait_time} seconds..."
            )
            time.sleep(wait_time)
        self.record_request(content_type)


class GeminiHandler:
    def __init__(self, get_api_key_func):
        """Initialize with a function that returns the next API key"""
        self.get_next_api_key = get_api_key_func
        self.rate_limiter = RateLimiter()
        self.current_key = None
        self.update_api_key()  # Initial configuration
        self.consecutive_429_errors = 0
        self.max_429_errors = (
            2  # Maximum consecutive 429 errors before forced key rotation
        )

    def update_api_key(self):
        """Update the Gemini configuration with a new API key"""
        new_key = self.get_next_api_key()

        # Only update if we got a different key
        if new_key != self.current_key:
            self.current_key = new_key
            genai.configure(api_key=new_key)
            self.analysis_model = genai.GenerativeModel("gemini-1.5-flash")
            self.embedding_model = "models/text-embedding-004"
            self.consecutive_429_errors = 0  # Reset error counter
            logger.info("API key rotated successfully")
            return True
        return False

    async def _handle_rate_limit(self, content_type: ContentType) -> bool:
        """Rotate API key on every rate limit error"""
        self.consecutive_429_errors += 1
        logger.warning("Rate limit hit, rotating API key...")

        if self.update_api_key():
            await asyncio.sleep(2)  # Brief pause after rotation
            return True

        # If key rotation failed, use exponential backoff
        wait_time = min(5 * (2**self.consecutive_429_errors), 30)
        logger.warning(f"Failed to rotate key, waiting {wait_time} seconds...")
        await asyncio.sleep(wait_time)
        return True

    def _get_embedding_prompt(self, content_type: ContentType) -> str:
        """Get appropriate prompt template based on content type"""
        prompts = {
            ContentType.PROJECT: """
            Analyze this creative project considering:
            - Visual style and aesthetic approach
            - Technical execution and creative techniques
            - Subject matter and narrative elements
            - Target audience and commercial application
            - Production value and professional execution
            Provide a comprehensive analysis that captures the project's essence and professional quality.
            """,
            ContentType.PORTFOLIO: """
            Analyze this creator's portfolio considering:
            - Signature style and creative identity
            - Technical expertise and specialized skills
            - Range of work and versatility
            - Industry focus and target markets
            - Professional standards and production quality
            Provide a holistic analysis of their work and professional capabilities.
            """,
            ContentType.CREATOR: """
            Analyze this creative professional's profile considering:
            - Core competencies and specializations
            - Industry positioning and target market
            - Professional background and expertise
            - Creative approach and style preferences
            Provide a comprehensive analysis of their professional identity.
            """,
        }
        return prompts.get(content_type, "")

    async def generate_content(
        self,
        prompt: str,
        context: str = "",
        content_type: ContentType = ContentType.IMAGE,
        generation_config: Optional[Dict] = None,
    ) -> Optional[str]:
        """Generate content with rate limiting and retries"""
        retries = 0
        max_retries = 3

        if generation_config is None:
            generation_config = {
                "temperature": 0.2,
                "top_k": 32,
                "top_p": 0.8,
                "max_output_tokens": 2048,
            }

        while retries < max_retries:
            try:
                logger.info(
                    f"Waiting for rate limit for {content_type.value} content..."
                )
                self.rate_limiter.wait_if_needed(content_type)

                logger.info("Generating content...")
                if content_type == ContentType.IMAGE:
                    # For image analysis, combine prompt and context
                    response = self.analysis_model.generate_content(
                        [prompt, context], generation_config=generation_config
                    )
                else:
                    # For other content types
                    response = self.analysis_model.generate_content(
                        [prompt, context], generation_config=generation_config
                    )

                if not response.text:
                    raise Exception("Empty response from Gemini")

                self.consecutive_429_errors = 0  # Reset on success
                return response.text.strip()

            except Exception as e:
                error_msg = str(e).lower()

                if (
                    "429" in error_msg
                    or "quota" in error_msg
                    or "rate limit" in error_msg
                ):
                    logger.warning(f"Rate limit error detected: {str(e)}")
                    if await self._handle_rate_limit(content_type):
                        continue  # Retry immediately after key rotation
                    else:
                        retries += 1
                else:
                    logger.error(
                        f"Error generating content (attempt {retries + 1}/{max_retries}): {str(e)}"
                    )
                    retries += 1
                    await asyncio.sleep(2)  # Brief pause before retry

                if retries >= max_retries:
                    logger.error("Max retries reached, giving up")
                    return None

        return None

    async def generate_embedding(
        self,
        content: Union[str, Dict, List[Dict]],
        content_type: ContentType,
        max_retries: int = 3,
    ) -> Optional[list]:
        """Generate embedding with improved rate limiting and retries"""
        retries = 0
        while retries < max_retries:
            try:
                logger.info(
                    f"Waiting for rate limit for {content_type.value} embedding..."
                )
                self.rate_limiter.wait_if_needed(content_type)

                # Prepare content based on type
                if content_type in [
                    ContentType.PROJECT,
                    ContentType.PORTFOLIO,
                    ContentType.CREATOR,
                ]:
                    prompt = self._get_embedding_prompt(content_type)
                    if isinstance(content, (dict, list)):
                        content = str(content)
                    embedding_text = f"{prompt}\n\n{content}"
                else:
                    if isinstance(content, dict) and "ai_analysis" in content:
                        embedding_text = content["ai_analysis"].get("description", "")
                    else:
                        embedding_text = str(content)

                logger.info("Generating embedding...")
                result = genai.embed_content(
                    model=self.embedding_model,
                    content={"text": embedding_text},
                    task_type="retrieval_document",
                )

                if "embedding" not in result:
                    raise Exception("No embedding in response")

                self.consecutive_429_errors = 0  # Reset on success
                return result["embedding"]

            except Exception as e:
                error_msg = str(e).lower()

                if (
                    "429" in error_msg
                    or "quota" in error_msg
                    or "rate limit" in error_msg
                ):
                    logger.warning(f"Rate limit error detected: {str(e)}")
                    if await self._handle_rate_limit(content_type):
                        continue  # Retry immediately after key rotation
                    else:
                        retries += 1
                else:
                    logger.error(
                        f"Error generating embedding (attempt {retries + 1}/{max_retries}): {str(e)}"
                    )
                    retries += 1
                    await asyncio.sleep(2)  # Brief pause before retry

                if retries >= max_retries:
                    logger.error("Max retries reached, giving up")
                    return None

        return None

    async def batch_generate_content(
        self,
        prompts_and_contexts: List[tuple],
        content_type: ContentType,
        batch_size: int = 5,
    ) -> List[Optional[str]]:
        """Generate content for multiple items in batches"""
        results = []

        for i in range(0, len(prompts_and_contexts), batch_size):
            batch = prompts_and_contexts[i : i + batch_size]
            batch_results = []

            for prompt, context in batch:
                result = await self.generate_content(prompt, context, content_type)
                batch_results.append(result)
                await asyncio.sleep(0.1)  # Small delay between items

            results.extend(batch_results)

        return results

    async def batch_generate_embeddings(
        self,
        items: List[Union[str, Dict]],
        content_type: ContentType,
        batch_size: int = 5,
    ) -> List[Optional[list]]:
        """Generate embeddings for multiple items in batches"""
        results = []

        for i in range(0, len(items), batch_size):
            batch = items[i : i + batch_size]
            batch_results = []

            for item in batch:
                embedding = await self.generate_embedding(item, content_type)
                batch_results.append(embedding)
                await asyncio.sleep(0.1)  # Small delay between items

            results.extend(batch_results)

        return results
